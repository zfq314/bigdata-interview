    简而言之，只要属于此窗口的第一个元素到达，就会创建一个窗口，当时间（事件或处理时间）超过其结束时间戳加上用户指定的允许延迟时，窗口将被完全删除。

    使用基于事件时间的窗口策略，每5分钟创建一个不重叠（或翻滚）的窗口并允许延迟1分钟。
    假定目前是12:00。
    当具有落入该间隔的时间戳的第一个元素到达时，Flink将为12:00到12:05之间的间隔创建一个新窗口，当水位线（watermark）到12:06时间戳时将删除它。

    窗口有如下组件：

    Window Assigner：用来决定某个元素被分配到哪个/哪些窗口中去。
    Trigger：触发器。决定了一个窗口何时能够被计算或清除。触发策略可能类似于“当窗口中的元素数量大于4”时，或“当水位线通过窗口结束时”。
    Evictor：它可以在 触发器触发后 & 应用函数之前和/或之后 从窗口中删除元素。
    窗口还拥有函数，比如 ProcessWindowFunction，ReduceFunction，AggregateFunction或FoldFunction。该函数将包含要应用于窗口内容的计算，而触发器指定窗口被认为准备好应用该函数的条件。

    事件时间程序必须指定如何生成事件时间的Watermarks，这是表示事件时间进度的机制。

    现在假设我们正在创建一个排序的数据流。这意味着应用程序处理流中的乱序到达的事件，并生成同样事件但按时间戳（事件时间）排序的新数据流。

    比如:

    有1~10个事件。
    乱序到达的序列是：1,2,4,5,6,3,8,9,10,7
    经过按 事件时间 处理后的序列是：1,2,3,4,5,6,7,8,9,10
    为了处理事件时间，Flink需要知道事件的时间戳，这意味着流中的每条数据都需要分配其事件时间戳。这通常通过提取每条数据中的固定字段来完成时间戳的获取。



    1. 窗口触发条件
    上面谈到了对数据乱序问题的处理机制是watermark+window，那么window什么时候该被触发呢？

    基于Event Time的事件处理，Flink默认的事件触发条件为：

    对于out-of-order及正常的数据而言

    watermark的时间戳 > = window endTime
    在 [window_start_time,window_end_time] 中有数据存在。
    对于late element太多的数据而言

    Event Time > watermark的时间戳
    WaterMark相当于一个EndLine，一旦Watermarks大于了某个window的end_time，就意味着windows_end_time时间和WaterMark时间相同的窗口开始计算执行了。
    就是说，我们根据一定规则，计算出Watermarks，并且设置一些延迟，给迟到的数据一些机会，也就是说正常来讲，对于迟到的数据，我只等你一段时间，再不来就没有机会了。
    WaterMark时间可以用Flink系统现实时间，也可以用处理数据所携带的Event time。
    使用Flink系统现实时间，在并行和多线程中需要注意的问题较少，因为都是以现实时间为标准。
    如果使用处理数据所携带的Event time作为WaterMark时间，需要注意两点：
    因为数据到达并不是循序的，注意保存一个当前最大时间戳作为WaterMark时间
    并行同步问题


    迟到事件
    虽说水位线表明着早于它的事件不应该再出现，但是上如上文所讲，接收到水位线以前的的消息是不可避免的，这就是所谓的迟到事件。实际上迟到事件是乱序事件的特例，和一般乱序事件不同的是它们的乱序程度超出了水位线的预计，导致窗口在它们到达之前已经关闭。
    迟到事件出现时窗口已经关闭并产出了计算结果，因此处理的方法有3种：
    重新激活已经关闭的窗口并重新计算以修正结果。
    将迟到事件收集起来另外处理。
    将迟到事件视为错误消息并丢弃。
    Flink 默认的处理方式是第3种直接丢弃，其他两种方式分别使用Side Output和Allowed Lateness。
    Side Output机制可以将迟到事件单独放入一个数据流分支，这会作为 window 计算结果的副产品，以便用户获取并对其进行特殊处理。
    Allowed Lateness机制允许用户设置一个允许的最大迟到时长。Flink 会在窗口关闭后一直保存窗口的状态直至超过允许迟到时长，这期间的迟到事件不会被丢弃，而是默认会触发窗口重新计算。因为保存窗口状态需要额外内存，并且如果窗口计算使用了 ProcessWindowFunction API 还可能使得每个迟到事件触发一次窗口的全量计算，代价比较大，所以允许迟到时长不宜设得太长，迟到事件也不宜过多，否则应该考虑降低水位线提高的速度或者调整算法。
    这里总结机制为：
    窗口window 的作用是为了周期性的获取数据。
    watermark的作用是防止数据出现乱序(经常)，事件时间内获取不到指定的全部数据，而做的一种保险方法。
    allowLateNess是将窗口关闭时间再延迟一段时间。
    sideOutPut是最后兜底操作，所有过期延迟数据，指定窗口已经彻底关闭了，就会把数据放到侧输出流。



    看看如何触发窗口
    我们明白了窗口的触发机制，这里我们添加了水位线，到底是个怎么个情况？我们来看下面

    假如我们设置10s的时间窗口（window），那么010s，1020s都是一个窗口，以0~10s为例，0为start-time，10为end-time。假如有4个数据的event-time分别是8(A),12.5(B),9(C),13.5(D)，
    我们设置Watermarks为当前所有到达数据event-time的最大值减去延迟值3.5秒

    当A到达的时候，Watermarks为max{8}-3.5=8-3.5 = 4.5 < 10,不会触发计算
    当B到达的时候，Watermarks为max(12.5,8)-3.5=12.5-3.5 = 9 < 10,不会触发计算
    当C到达的时候，Watermarks为max(12.5,8,9)-3.5=12.5-3.5 = 9 < 10,不会触发计算
    当D到达的时候，Watermarks为max(13.5,12.5,8,9)-3.5=13.5-3.5 = 10 = 10,触发计算
    触发计算的时候，会将A，C（因为他们都小于10）都计算进去，其中C是迟到的。
    max这个很关键，就是当前窗口内，所有事件的最大事件。
    这里的延迟3.5s是我们假设一个数据到达的时候，比他早3.5s的数据肯定也都到达了，这个是需要根据经验推算。假设加入D到达以后有到达了一个E,event-time=6，但是由于0~10的时间窗口已经开始计算了，所以E就丢了。
    从这里上面E的丢失说明，水位线也不是万能的，但是如果根据我们自己的生产经验+侧道输出等方案，可以做到数据不丢失。


     * //TODO  forBoundedOutOfOrderness 最大延迟时间，eventTime-forBoundedOutOfOrderness 来判断窗口是否触发计算[
     * // TODO  窗口规则 左闭右开
     * //TODO   senser,2,11  2-3<10 不触发
     * //TODO   senser,6,11 6-3<10 不触发
     * // TODO  senser,3,11 3-3<10 不触发
     * //TODO  senser,10,11 10-3<10 不触发
     * //TODO  senser,13,11 13-3=10 触发计算 但是不关闭窗口 ，关闭窗口的时间，是当前的watermark+Allowedlateness,此时的watermark=13,+2 15,
     * //TODO  15的时候才会关闭，后面来的数据需要进侧输出流，进行数据的兜底操作 后面都是下个窗口


     基于时间的合流——双流联结（Join）
     可以发现，根据某个key合并两条流，与关系型数据库中表的join操作非常相近。事实上，Flink中两条流的connect操作，就可以通过keyBy指定键进行分组后合并，实现了类似于SQL中的join操作；
     另外connect支持处理函数，可以使用自定义实现各种需求，其实已经能够处理双流join的大多数场景。
     不过处理函数是底层接口，所以尽管connect能做的事情多，但在一些具体应用场景下还是显得太过抽象了。
     比如，如果我们希望统计固定时间内两条流数据的匹配情况，那就需要自定义来实现——其实这完全可以用窗口（window）来表示。为了更方便地实现基于时间的合流操作，Flink的DataStrema API提供了内置的join算子


     窗口联结的调用
     窗口联结在代码中的实现，首先需要调用DataStream的.join()方法来合并两条流，得到一个JoinedStreams；接着通过.where()和.equalTo()方法指定两条流中联结的key；
     然后通过.window()开窗口，并调用.apply()传入联结窗口函数进行处理计算。通用调用形式如下：
     stream1.join(stream2)
             .where(<KeySelector>)
             .equalTo(<KeySelector>)
             .window(<WindowAssigner>)
             .apply(<JoinFunction>)
     上面代码中.where()的参数是键选择器（KeySelector），用来指定第一条流中的key；而.equalTo()传入的KeySelector则指定了第二条流中的key。
     两者相同的元素，如果在同一窗口中，就可以匹配起来，并通过一个“联结函数”（JoinFunction）进行处理了。
     这里.window()传入的就是窗口分配器，之前讲到的三种时间窗口都可以用在这里：滚动窗口（tumbling window）、滑动窗口（sliding window）和会话窗口（session window）。
     而后面调用.apply()可以看作实现了一个特殊的窗口函数。注意这里只能调用.apply()，没有其他替代的方法。
     传入的JoinFunction也是一个函数类接口，使用时需要实现内部的.join()方法。这个方法有两个参数，分别表示两条流中成对匹配的数据。
     其实仔细观察可以发现，窗口join的调用语法和我们熟悉的SQL中表的join非常相似：
     SELECT * FROM table1 t1, table2 t2 WHERE t1.id = t2.id;
     这句SQL中where子句的表达，等价于inner join ... on，所以本身表示的是两张表基于id的“内连接”（inner join）。而Flink中的window join，同样类似于inner join。
     也就是说，最后处理输出的，只有两条流中数据按key配对成功的那些；
     如果某个窗口中一条流的数据没有任何另一条流的数据匹配，那么就不会调用JoinFunction的.join()方法，也就没有任何输出了。

     间隔联结（Interval Join）
     在有些场景下，我们要处理的时间间隔可能并不是固定的。这时显然不应该用滚动窗口或滑动窗口来处理——因为匹配的两个数据有可能刚好“卡在”窗口边缘两侧，
     于是窗口内就都没有匹配了；会话窗口虽然时间不固定，但也明显不适合这个场景。基于时间的窗口联结已经无能为力了。
     为了应对这样的需求，Flink提供了一种叫作“间隔联结”（interval join）的合流操作。顾名思义，间隔联结的思路就是针对一条流的每个数据，
     开辟出其时间戳前后的一段时间间隔，看这期间是否有来自另一条流的数据匹配。
     1）间隔联结的原理
     间隔联结具体的定义方式是，我们给定两个时间点，分别叫作间隔的“上界”（upperBound）和“下界”（lowerBound）；
     于是对于一条流（不妨叫作A）中的任意一个数据元素a，就可以开辟一段时间间隔：[a.timestamp + lowerBound, a.timestamp + upperBound],即以a的时间戳为中心，
     下至下界点、上至上界点的一个闭区间：我们就把这段时间作为可以匹配另一条流数据的“窗口”范围。所以对于另一条流（不妨叫B）中的数据元素b，
     如果它的时间戳落在了这个区间范围内，a和b就可以成功配对，进而进行计算输出结果。所以匹配的条件为：
     a.timestamp + lowerBound <= b.timestamp <= a.timestamp + upperBound
     这里需要注意，做间隔联结的两条流A和B，也必须基于相同的key；下界lowerBound应该小于等于上界upperBound，两者都可正可负；间隔联结目前只支持事件时间语义。

     下方的流A去间隔联结上方的流B，所以基于A的每个数据元素，都可以开辟一个间隔区间。我们这里设置下界为-2毫秒，上界为1毫秒。于是对于时间戳为2的A中元素，它的可匹配区间就是[0, 3],流B中有时间戳为0、1的两个元素落在这个范围内，所以就可以得到匹配数据对（2, 0）和（2, 1）。同样地，A中时间戳为3的元素，可匹配区间为[1, 4]，B中只有时间戳为1的一个数据可以匹配，于是得到匹配数据对（3, 1）。
     所以我们可以看到，间隔联结同样是一种内连接（inner join）。与窗口联结不同的是，interval join做匹配的时间段是基于流中数据的，所以并不确定；而且流B中的数据可以不只在一个区间内被匹配。
     2）间隔联结的调用
     间隔联结在代码中，是基于KeyedStream的联结（join）操作。DataStream在keyBy得到KeyedStream之后，可以调用.intervalJoin()来合并两条流，传入的参数同样是一个KeyedStream，两者的key类型应该一致；得到的是一个IntervalJoin类型。后续的操作同样是完全固定的：先通过.between()方法指定间隔的上下界，再调用.process()方法，定义对匹配数据对的处理操作。调用.process()需要传入一个处理函数，这是处理函数家族的最后一员：“处理联结函数”ProcessJoinFunction。
     通用调用形式如下：
     stream1
         .keyBy(<KeySelector>)
         .intervalJoin(stream2.keyBy(<KeySelector>))
         .between(Time.milliseconds(-2), Time.milliseconds(1))
         .process (new ProcessJoinFunction<Integer, Integer, String(){

             @Override
             public void processElement(Integer left, Integer right, Context ctx, Collector<String> out) {
                 out.collect(left + "," + right);
             }
         });
     可以看到，抽象类ProcessJoinFunction就像是ProcessFunction和JoinFunction的结合，
     内部同样有一个抽象方法.processElement()。与其他处理函数不同的是，它多了一个参数，这自然是因为有来自两条流的数据。
     参数中left指的就是第一条流中的数据，right则是第二条流中与它匹配的数据。每当检测到一组匹配，就会调用这里的.processElement()方法，经处理转换之后输出结果。