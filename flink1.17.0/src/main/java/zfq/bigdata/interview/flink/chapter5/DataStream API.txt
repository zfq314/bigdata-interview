1 执行环境（Execution Environment）
    Flink程序可以在各种上下文环境中运行：我们可以在本地JVM中执行程序，也可以提交到远程集群上运行。
    不同的环境，代码的提交运行的过程会有所不同。这就要求我们在提交作业执行计算时，首先必须获取当前Flink的运行环境，从而建立起与Flink框架之间的联系。

    创建执行环境
    我们要获取的执行环境，是StreamExecutionEnvironment类的对象，这是所有Flink程序的基础。在代码中创建执行环境的方式，就是调用这个类的静态方法，具体有以下三种。
    1）getExecutionEnvironment
    最简单的方式，就是直接调用getExecutionEnvironment方法。它会根据当前运行的上下文直接得到正确的结果：如果程序是独立运行的，就返回一个本地执行环境；
    如果是创建了jar包，然后从命令行调用它并提交到集群执行，那么就返回集群的执行环境。也就是说，这个方法会根据当前运行的方式，自行决定该返回什么样的运行环境。
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    这种方式，用起来简单高效，是最常用的一种创建执行环境的方式。
    2）createLocalEnvironment
    这个方法返回一个本地执行环境。可以在调用时传入一个参数，指定默认的并行度；如果不传入，则默认并行度就是本地的CPU核心数。
    StreamExecutionEnvironment localEnv = StreamExecutionEnvironment.createLocalEnvironment();
    3）createRemoteEnvironment
    这个方法返回集群执行环境。需要在调用时指定JobManager的主机名和端口号，并指定要在集群中运行的Jar包。
    StreamExecutionEnvironment remoteEnv = StreamExecutionEnvironment
      		.createRemoteEnvironment(
        		"host",                   // JobManager主机名
        		1234,                     // JobManager进程端口号
       			"path/to/jarFile.jar"  // 提交给JobManager的JAR包
    		);
    在获取到程序执行环境后，我们还可以对执行环境进行灵活的设置。比如可以全局设置程序的并行度、禁用算子链，还可以定义程序的时间语义、配置容错机制。

执行模式（Execution Mode）
    DataStream API执行模式包括：流执行模式、批执行模式和自动模式。
    流执行模式（Streaming）
    这是DataStream API最经典的模式，一般用于需要持续实时处理的无界数据流。默认情况下，程序使用的就是Streaming执行模式。
    批执行模式（Batch）
    专门用于批处理的执行模式。
    自动模式（AutoMatic）
    在这种模式下，将由程序根据输入数据源是否有界，来自动选择执行模式。
    批执行模式的使用。主要有两种方式：
    （1）通过命令行配置
    bin/flink run -Dexecution.runtime-mode=BATCH ...
    在提交作业时，增加execution.runtime-mode参数，指定值为BATCH。
    （2）通过代码配置
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

    env.setRuntimeMode(RuntimeExecutionMode.BATCH);
    在代码中，直接基于执行环境调用setRuntimeMode方法，传入BATCH模式。
    实际应用中一般不会在代码中配置，而是使用命令行，这样更加灵活。

触发程序执行
    需要注意的是，写完输出（sink）操作并不代表程序已经结束。因为当main()方法被调用时，其实只是定义了作业的每个执行操作，然后添加到数据流图中；
    这时并没有真正处理数据——因为数据可能还没来。Flink是由事件驱动的，只有等到数据到来，才会触发真正的计算，这也被称为“延迟执行”或“懒执行”。
    所以我们需要显式地调用执行环境的execute()方法，来触发程序执行。execute()方法将一直等待作业完成，然后返回一个执行结果（JobExecutionResult）。
    env.execute();

Flink支持的数据类型
    1）Flink的类型系统
    Flink使用“类型信息”（TypeInformation）来统一表示数据类型。TypeInformation类是Flink中所有类型描述符的基类。它涵盖了类型的一些基本属性，并为每个数据类型生成特定的序列化器、反序列化器和比较器。

    2）Flink支持的数据类型
    对于常见的Java和Scala数据类型，Flink都是支持的。Flink在内部，Flink对支持不同的类型进行了划分，这些类型可以在Types工具类中找到：
    （1）基本类型
    所有Java基本类型及其包装类，再加上Void、String、Date、BigDecimal和BigInteger。
    （2）数组类型
    包括基本类型数组（PRIMITIVE_ARRAY）和对象数组（OBJECT_ARRAY）。
    （3）复合数据类型
    Java元组类型（TUPLE）：这是Flink内置的元组类型，是Java API的一部分。最多25个字段，也就是从Tuple0~Tuple25，不支持空字段。
    Scala 样例类及Scala元组：不支持空字段。
    行类型（ROW）：可以认为是具有任意个字段的元组，并支持空字段。
    POJO：Flink自定义的类似于Java bean模式的类。
    （4）辅助类型
    Option、Either、List、Map等。
    （5）泛型类型（GENERIC）
    Flink支持所有的Java类和Scala类。不过如果没有按照上面POJO类型的要求来定义，就会被Flink当作泛型类来处理。Flink会把泛型类型当作黑盒，无法获取它们内部的属性；它们也不是由Flink本身序列化的，而是由Kryo序列化的。
    在这些类型中，元组类型和POJO类型最为灵活，因为它们支持创建复杂类型。而相比之下，POJO还支持在键（key）的定义中直接使用字段名，这会让我们的代码可读性大大增加。所以，在项目实践中，往往会将流处理程序中的元素类型定为Flink的POJO类型。
    Flink对POJO类型的要求如下：
    类是公有（public）的
    有一个无参的构造方法
    所有属性都是公有（public）的
    所有属性的类型都是可以序列化的
    3）类型提示（Type Hints）
    Flink还具有一个类型提取系统，可以分析函数的输入和返回类型，自动获取类型信息，从而获得对应的序列化器和反序列化器。但是，由于Java中泛型擦除的存在，在某些特殊情况下（比如Lambda表达式中），自动提取的信息是不够精细的——只告诉Flink当前的元素由“船头、船身、船尾”构成，根本无法重建出“大船”的模样；这时就需要显式地提供类型信息，才能使应用程序正常工作或提高其性能。
    为了解决这类问题，Java API提供了专门的“类型提示”（type hints）。
    回忆一下之前的word count流处理程序，我们在将String类型的每个词转换成（word， count）二元组后，就明确地用returns指定了返回的类型。因为对于map里传入的Lambda表达式，系统只能推断出返回的是Tuple2类型，而无法得到Tuple2<String, Long>。只有显式地告诉系统当前的返回类型，才能正确地解析出完整数据。
    .map(word -> Tuple2.of(word, 1L))
    .returns(Types.TUPLE(Types.STRING, Types.LONG));
    Flink还专门提供了TypeHint类，它可以捕获泛型的类型信息，并且一直记录下来，为运行时提供足够的信息。我们同样可以通过.returns()方法，明确地指定转换之后的DataStream里元素的类型。
    returns(new TypeHint<Tuple2<Integer, SomeType>>(){})

    简单聚合（sum/min/max/minBy/maxBy）
    有了按键分区的数据流KeyedStream，我们就可以基于它进行聚合操作了。Flink为我们内置实现了一些最基本、最简单的聚合API，主要有以下几种：
    sum()：在输入流上，对指定的字段做叠加求和的操作。
    min()：在输入流上，对指定的字段求最小值。
    max()：在输入流上，对指定的字段求最大值。
    minBy()：与min()类似，在输入流上针对指定字段求最小值。不同的是，min()只计算指定字段的最小值，其他字段会保留最初第一个数据的值；而minBy()则会返回包含字段最小值的整条数据。
    maxBy()：与max()类似，在输入流上针对指定字段求最大值。两者区别与min()/minBy()完全一致。
    简单聚合算子使用非常方便，语义也非常明确。这些聚合方法调用时，也需要传入参数；但并不像基本转换算子那样需要实现自定义函数，只要说明聚合指定的字段就可以了。指定字段的方式有两种：指定位置，和指定名称。



    用户自定义函数（UDF）
    用户自定义函数（user-defined function，UDF），即用户可以根据自身需求，重新实现算子的逻辑。
    用户自定义函数分为：函数类、匿名函数、富函数类。


    富函数类（Rich Function Classes）
    “富函数类”也是DataStream API提供的一个函数类的接口，所有的Flink函数类都有其Rich版本。富函数类一般是以抽象类的形式出现的。例如：RichMapFunction、RichFilterFunction、RichReduceFunction等。
    与常规函数类的不同主要在于，富函数类可以获取运行环境的上下文，并拥有一些生命周期方法，所以可以实现更复杂的功能。
    Rich Function有生命周期的概念。典型的生命周期方法有：
    open()方法，是Rich Function的初始化方法，也就是会开启一个算子的生命周期。当一个算子的实际工作方法例如map()或者filter()方法被调用之前，open()会首先被调用。
    close()方法，是生命周期中的最后一个调用的方法，类似于结束方法。一般用来做一些清理工作。
    需要注意的是，这里的生命周期方法，对于一个并行子任务来说只会调用一次；而对应的，实际工作方法，例如RichMapFunction中的map()，在每条数据到来后都会触发一次调用。

    物理分区算子（Physical Partitioning）
    常见的物理分区策略有：随机分配（Random）、轮询分配（Round-Robin）、重缩放（Rescale）和广播（Broadcast）。

    随机分区（shuffle）
    最简单的重分区方式就是直接“洗牌”。通过调用DataStream的.shuffle()方法，将数据随机地分配到下游算子的并行任务中去。
    随机分区服从均匀分布（uniform distribution），所以可以把流中的数据随机打乱，均匀地传递到下游任务分区。因为是完全随机的，所以对于同样的输入数据, 每次执行得到的结果也不会相同。

    轮询分区（Round-Robin）
    轮询，简单来说就是“发牌”，按照先后顺序将数据做依次分发。通过调用DataStream的.rebalance()方法，就可以实现轮询重分区。rebalance使用的是Round-Robin负载均衡算法，可以将输入流数据平均分配到下游的并行任务中去。

    重缩放分区（rescale）
    重缩放分区和轮询分区非常相似。当调用rescale()方法时，其实底层也是使用Round-Robin算法进行轮询，但是只会将数据轮询发送到下游并行任务的一部分中。rescale的做法是分成小团体，发牌人只给自己团体内的所有人轮流发牌。

    广播（broadcast）
    这种方式其实不应该叫做“重分区”，因为经过广播之后，数据会在不同的分区都保留一份，可能进行重复处理。可以通过调用DataStream的broadcast()方法，将输入数据复制并发送到下游算子的所有并行任务中去。
    stream.broadcast()

    全局分区（global）
    全局分区也是一种特殊的分区方式。这种做法非常极端，通过调用.global()方法，会将所有的输入流数据都发送到下游算子的第一个并行子任务中去。这就相当于强行让下游任务并行度变成了1，所以使用这个操作需要非常谨慎，可能对程序造成很大的压力。
    stream.global()

    分流
    所谓“分流”，就是将一条数据流拆分成完全独立的两条、甚至多条流。也就是基于一个DataStream，定义一些筛选条件，将符合条件的数据拣选出来放到对应的流里。