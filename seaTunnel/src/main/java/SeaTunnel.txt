SeaTunnel是什么?
    SeaTunnel是一个简单易用的数据集成框架，在企业中，由于开发时间或开发部门不通用，往往有多个异构的、运行在不同的软硬件平台上的信息系统同时运行。
    数据集成是把不同来源、格式、特点性质的数据在逻辑上或物理上有机地集中，从而为企业提供全面的数据共享。
    SeaTunnel支持海量数据的实时同步。它每天可以稳定高效地同步数百亿数据。并已用于近100家公司的生产。

SeaTunnel在做什么
    本质上，SeaTunnel不是对Saprk和Flink的内部修改，而是在Spark和Flink的基础上做了一层包装。它主要运用了控制反转的设计模式，这也是SeaTunnel实现的基本思想。
    SeaTunnel的日常使用，就是编辑配置文件。编辑好的配置文件由SeaTunnel转换为具体的Spark或Flink任务

应用场景
    SeaTunnel适用于以下场景
        海量数据的同步
        海量数据的集成
        海量数据的ETL
        海量数据聚合
        多源数据处理
    SeaTunnel的特点
        基于配置的低代码开发，易用性高，方便维护。
        支持实时流式传输
        离线多源数据分析
        高性能、海量数据处理能力
        模块化的插件架构，易于扩展
        支持用SQL进行数据操作和数据聚合
        支持Spark structured streaming
        支持Spark 2.x

        目前SeaTunnel的长板是他有丰富的连接器，又因为它以Spark和Flink为引擎。所以可以很好地进行分布式的海量数据同步。
        通常SeaTunnel会被用来做出仓入仓工具，或者被用来进行数据集成。比如，唯品会就选择用SeaTunnel来解决数据孤岛问题，让ClcikHouse集成到了企业中先前的数据系统之中

    apache-seatunnel-incubating-2.1.0的目录就是我们已经安装好的seatunnel。Incubating的意思是孵化中。

    config/目录中有一个seatunnel-env.sh

    # Home directory of spark distribution.
    SPARK_HOME=${SPARK_HOME:-/opt/spark}
    # Home directory of flink distribution.
    FLINK_HOME=${FLINK_HOME:-/opt/flink}

    这个脚本中声明了SPARK_HOME和FLINK_HOME两个路径。默认情况下seatunnel-env.sh中的SPARK_HOME和FLINK_HOME就是系统环境变量中的SPARK_HOME和FLINK_HOME。
    在shell脚本中:-的意思是如果:-前的内容为空，则替换为后面的。
    例如，环境变量中没有FLINK_HOME。那么SeaTunnel运行时会将FLINK_HOME设为/opt/flink。
    如果你机器上的环境变量SPARK_HOME指向了3.x的一个版本。但是想用2.x的Spark来试一下SeaTunnel。这种情况下，如果你不想改环境变量，那就直接在seatunnel-env.sh中将2.x的路径赋值给SPARK_HOME即可。

SeaTunnel基本原理

     先启动flink集群 ，flink1.17.0有点问题，用的1.13.0版本是没有问题的


    SeaTunnel的启动脚本
    启动脚本的参数
    截至目前，SeaTunnel有两个启动脚本。
    提交spark任务用start-seatunnel-spark.sh。
    提交flink任务则用start-seatunnel-flink.sh。
    本文档主要是结合flink来使用seatunnel的，所以用start-seatunnel-flink.sh来讲解。
    start-seatunnle-flink.sh可以指定3个参数
        分别是：
            --config 应用配置的路径
            --variable 应用配置里的变量赋值
            --check	检查config语法是否合法
         --check参数
        截至本文档撰写时的SeaTunnel版本v2.1.0。check功能还尚在开发中，因此--check参数是一个虚设。目前start-seatunnel-flink.sh并不能对应用配置文件的语法合法性进行检查。
        而且start-seatunnel-flink.sh中目前没有对--check参数的处理逻辑。
        需要注意！使用过程中，如果没有使用--check参数，命令行一闪而过。那就是你的配置文件语法有问题。

        --config参数和--variable参数
        --config参数用来指定应用配置文件的路径。
        --variable参数可以向配置文件传值。配置文件内是支持声明变量的。然后我们可以通过命令行给配置中的变量赋值。
        变量声明语法如下。
        sql {
            sql = "select * from (select info,split(info) from fake) where age > '"${age}"'"
          }
        在配置文件的任何位置都可以声明变量。并用命令行参数--variable key=value的方式将变量值传进去，你也可以用它的短命令形式 -i key=value。传递参数时，key需要和配置文件中声明的变量名保持一致。
        如果需要传递多个参数，那就在命令行里面传递多个-i或--variable key=value。
        比如：

        bin/start-seatunnel-flink.sh --config/xxx.sh -i age=18 -i sex=man
        bin/start-seatunnel-flink.sh --config config/example01.sh -i age=18



        脚本1
        参数： zhangsan#18

        脚本 ./bin/start-seatunnel-flink.sh --config config/example01.conf

        是SeaTunnel帮我们把配置文件翻译为具体的flink任务。配置化，低代码，易维护是SeaTunnel最显著的特点。


        脚本2：
        参数：zhangsan#21
             lisi#12 过滤掉
         bin/start-seatunnel-flink.sh --config config/example02.sh -i age=18




    我们可知，凡是--config 和 --variable之外的命令行参数都被放到PARAMS变量中，最后相当于给flink run传递了参数。注意！命令行参数解析过程中没有涉及--check参数处理。这也是为什么说它目前不支持--check操作。
    我们可以在seatunnel启动脚本中，指定flink job并行度。
    bin/start-seatunnel-flink.sh --config config/ -p 2\


SeaTunnel的配置文件
    应用配置的4个基本组件
    我们从SeaTunnel的app配置文件开始讲起。
    一个完整的SeaTunnel配置文件应包含四个配置组件。分别是：
    env{}   source{} --> transform{} --> sink{}

     env块
    env块中可以直接写spark或flink支持的配置项。比如并行度，检查点间隔时间。检查点hdfs路径等。在SeaTunnel源码的ConfigKeyName类中，声明了env块中所有可用的key。

    source块
    source块是用来声明数据源的。source块中可以声明多个连接器。比如：
    # 伪代码
    env {
        ...
    }

    source {
      hdfs { ... }
      elasticsearch { ... }
      jdbc {...}
    }

    transform {
        sql {
         sql = """
            select .... from hdfs_table
            join es_table
            on hdfs_table.uid = es_table.uid where ..."""
        }
    }

    sink {
        elasticsearch { ... }
    }
    需要注意的是，所有的source插件中都可以声明result_table_name。如果你声明了result_table_name。
    SeaTunnel会将source插件输出的DataStream<Row>转换为Table并注册在Table环境中。
    当你指定了result_table_name，那么你还可以指定field_name，在注册时，给Table重设字段名(后面的案例中会讲解)。


   transform块
   目前社区对插件做了很多规划，但是截至v2.1.0版本，可用的插件总共有两个，一个是Split，另一个是sql。
   transform{}块中可以声明多个转换插件。所有的转换插件都可以使用source_table_name，和result_table_name。同样，如果我们声明了result_table_name，那么我们就能声明field_name。
   我们需要着重了解一下Split插件和sql插件的实现。


   sink块
   Sink块里可以声明多个sink插件，每个sink插件都可以指定source_table_name。不过因为不同Sink插件的配置差异较大，所以在实现时建议参考官方文档。

   SeaTunnel的基本原理
   SeaTunnel的工作原理简单明了。
   1）程序会解析你的应用配置，并创建环境
   2）配置里source{}，transform{}，sink{}三个块中的插件最终在程序中以List集合的方式存在。
   3）由Excution对象来拼接各个插件，这涉及到选择source_table，注册result_table等流程，注册udf等流程。并最终触发执行

   kafka kafka-server-start.sh 修改 JMX_PORT 修改监控端口

    if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
        export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
        export JMX_PORT="9999"
    fi

   Kafka进Kafka出的简单ETL

   1）首先，创建为kafka创建test_csv主题。
   kafka-topics.sh --zookeeper hadoop31:2181 --create --topic test_csv --partitions 1 --replication-factor 1


   2）为kafka创建test_sink主题
   kafka-topics.sh --zookeeper hadoop31:2181 --create --topic test_sink --partitions 1 --replication-factor 1

   example03.conf
   env {
     # You can set flink configuration here
     execution.parallelism = 1
     #execution.checkpoint.interval = 10000
     #execution.checkpoint.data-uri = "hdfs://hadoop102:9092/checkpoint"
   }

   # 在source所属的块中配置数据源
   source {
       KafkaTableStream {
           consumer.bootstrap.servers = "hadoop31:9092"
           consumer.group.id = "seatunnel-learn"
           topics = test_csv
           result_table_name = test
           format.type = csv
           schema = "[{\"field\":\"name\",\"type\":\"string\"},{\"field\":\"age\", \"type\": \"int\"}]"
           format.field-delimiter = ";"
           format.allow-comments = "true"
           format.ignore-parse-errors = "true"
       }
   }
   # 在transform的块中声明转换插件
   transform {

     sql {
       sql = "select name,age from test  where age > '"${age}"'"
     }
   }
   # 在sink块中声明要输出到哪
   sink {
      kafkaTable {
       topics = "test_sink"
       producer.bootstrap.servers = "hadoop31:9092"
           }
   }

  这块需要注意目录
 /data/program/apache-seatunnel-incubating-2.1.0
  启动
 ./bin/start-seatunnel-flink.sh --config config/example03.conf -i age=18

  //启动生产者
     kafka-console-producer.sh --broker-list hadoop31:9092 --topic test_csv

  //启动消费者
     kafka-console-consumer.sh --bootstrap-server hadoop31:9092 --topic test_sink --from-beginning


  Kafka 输出到Doris进行指标统计
  生成伪数据脚本

fake.py
    from datetime import datetime
    from datetime import timedelta
    from datetime import date
    import random
    from time import sleep
    from faker import Faker
    from kafka import KafkaProducer
    import json
    import getopt
    import sys

    fake = Faker(locale='zh-CN')

    def generate_time_series(start, end):
        current = datetime.strptime(start, '%Y-%m-%d %H:%M:%S')
        end_item = datetime.strptime(end, '%Y-%m-%d %H:%M:%S')
        time_delta = timedelta(minutes=10)
        time_series = [current]
        while current < end_item:
            next = current + time_delta
            time_series.append(next)
            current = next
        return time_series

    class VideoSession:

        def __init__(self,start_time):
            self.start_time = start_time
            self.session_id = fake.uuid4()
            self.count = 0
            self.last_time = start_time

    class User:

        def __init__(self,id,city,id_card) :
            self.id = id
            self.city = city
            self.id_card = id_card
            self.session = None


        @property
        def age(self):

            birth_year = int(self.id_card[6:10])
            today_year = date.today().year
            how_old = today_year - birth_year
            return how_old

        def __getattribute__(self, __name):
                if __name == "session_id":
                    count = count + 1
                    if(count>3):
                        self.session = None
                    return object.__getattribute__(self,__name)
                return object.__getattribute__(self,__name)

        def __str__(self):
            return {
                "user_id": self.user_id,
                "city": self.city,
                "age": self.age
            }.__str__()

    if __name__ == "__main__":
        opts, args = getopt.getopt(sys.argv[1:],shortopts="b:t:",longopts=["bootstrap-server=", "topic="])
        opts = dict(opts)
        producer = KafkaProducer(bootstrap_servers=opts.get("--bootstrap-server"),value_serializer=lambda m: json.dumps(m,ensure_ascii=False).encode(encoding="utf-8"))
        user_list = [User(i,fake.city(),fake.ssn()) for i in range(1000)]
        time_seris = generate_time_series("2022-03-21 08:00:00", "2022-03-21 21:00:00")
        for current in time_seris:
            sampled = random.sample(user_list,200)
            for user in sampled:
                if(user.session is None):
                    user.session = VideoSession(current)
                    user.session.count += random.randint(2,10)
                elif (current - user.session.last_time <= timedelta(minutes=30)):
                    user.session.count += random.randint(2,10)
                    user.session.last_time = current
                else :
                    data = {
                        "session_id": user.session.session_id,
                        "video_count": user.session.count,
                        "duration_time": int((timedelta(minutes=20) if(user.session.last_time == user.session.start_time) else user.session.last_time - user.session.start_time).total_seconds()),
                        "user_id": user.id,
                        "user_age": user.age,
                        "city": user.city,
                        "session_start_time": user.session.start_time.__str__(),
                        "session_end_time": user.session.last_time.__str__()
                    }
                    producer.send(opts.get("--topic"),data)
                    user.session = None
            sleep(1)













  pip3 install Faker

  pip3 install kafka-python

  ）使用mysql客户端连接doris
  [atguigu@hadoop102 fake_data]$ mysql -h hadoop102 -P 9030 -uzfq  -p123321

  create database test_db;


  CREATE TABLE `example_user_video` (
    `user_id` largeint(40) NOT NULL COMMENT "用户id",
    `city` varchar(20) NOT NULL COMMENT "用户所在城市",
    `age` smallint(6) NULL COMMENT "用户年龄",
    `video_sum` bigint(20) SUM NULL DEFAULT "0" COMMENT "总观看视频数",
    `max_duration_time` int(11) MAX NULL DEFAULT "0" COMMENT "用户最长会话时长",
    `min_duration_time` int(11) MIN NULL DEFAULT "999999999" COMMENT "用户最小会话时长",
    `last_session_date` datetime REPLACE NULL DEFAULT "1970-01-01 00:00:00" COMMENT "用户最后一次会话时间"
  ) ENGINE=OLAP
  AGGREGATE KEY(`user_id`, `city`, `age`)
  COMMENT "OLAP"
  DISTRIBUTED BY HASH(`user_id`) BUCKETS 16
  ;

7）使用python脚本向kafka中生成伪数据
python3 fake_video.py --bootstrap-server hadoop102:9092 --topic test_video

8）查看doris中的结果。
Select * from `example_user_video`;